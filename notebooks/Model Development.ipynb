{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Model Development\n",
        "Below is a set of steps to prepare the pipeline for model development\n",
        "\n",
        "### 1. Import libraries and load data from database.\n",
        "* Import libraries to be used across the notebook\n",
        "\n",
        "* Load dataset from database with `read_sql_table`\n",
        "\n",
        "* Define feature and target variables X and Y\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6qlm9qPf5ffA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "\n",
        "import re\n",
        "\n",
        "from sklearn.multioutput import MultiOutputClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "from sqlalchemy import create_engine"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5y43Up6L5yS9",
        "outputId": "ec35adcc-d897-4234-9978-afe3b5191684"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data from database\n",
        "engine = create_engine('sqlite:///..data/datasets.db')\n",
        "preprocessed_dataset = pd.read_sql_table('labelled_messages', engine) \n",
        "X = preprocessed_dataset['message']\n",
        "Y = preprocessed_dataset.drop(['message', 'original', 'genre', 'id'], axis=1)"
      ],
      "metadata": {
        "id": "APHGUdiUePHa"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Define a tokenization function to process the text data"
      ],
      "metadata": {
        "id": "hrnVwqpgfJ0D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(text):\n",
        "    tokens = re.sub(r\"[^a-zA-Z0-9]\", \" \", text.lower())\n",
        "    \n",
        "    tokens = text.split()\n",
        "    \n",
        "    tokens = [t for t in tokens if t not in stopwords.words(\"english\")]\n",
        "    \n",
        "    # Reduce words to their stems\n",
        "    tokens = [PorterStemmer().stem(t) for t in tokens]\n",
        "    \n",
        "    # Reduce words to their root form\n",
        "    tokens = [WordNetLemmatizer().lemmatize(t) for t in tokens]\n",
        "    \n",
        "    return tokens\n",
        "\n",
        "print(tokenize('Is the Hurricane over or is it not over'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V0uFytYkeqXI",
        "outputId": "8f9a2e0b-7ef0-4bb4-8346-0efa55315250"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Is', 'hurrican']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Build the machine learning pipeline\n",
        "This machine pipeline should take in the `message` column as input and output classification results on the other 36 categories in the dataset."
      ],
      "metadata": {
        "id": "N5PdOvsmfQUN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We first use the RandomForestClassifier for the multi-output prediction\n",
        "pipeline = Pipeline([\n",
        "    ('vect1', CountVectorizer(tokenizer=tokenize)), # Adding a count vectorizer that utilizes the previously defined tokenize function\n",
        "    ('tfidf1', TfidfTransformer()), # Using a term frequency inverse document frequency transformer to construct feature matrix\n",
        "    ('clf1', MultiOutputClassifier(RandomForestClassifier()))\n",
        "])"
      ],
      "metadata": {
        "id": "lmZvXovFfVUx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Train machine learning model for multi-output classification\n",
        "- Split data into train and test sets\n",
        "- Train the model with default parameters"
      ],
      "metadata": {
        "id": "NZ-BJuMwfaNS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the dataset into training and testing datasets\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y)\n",
        "\n",
        "# Fit the previously defined pipeline with default parameter on the training dataset\n",
        "pipeline.fit(X_train, Y_train, verbose=2)\n",
        "\n",
        "# Predict on test data\n",
        "Y_pred = pipeline.predict(X_test)\n",
        "\n",
        "# Evaluate model accuracy\n",
        "labels = np.unique(Y_pred)\n",
        "accuracy = (Y_pred == Y_test).mean()\n",
        "\n",
        "print(\"Labels:\", labels)\n",
        "print(\"Accuracy:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lAuiLRMqfi4R",
        "outputId": "91bbeef2-4195-4cef-de96-2c954b328a7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Labels: [0 1]\n",
            "Accuracy: related                   0.817912\n",
            "request                   0.885005\n",
            "offer                     0.995163\n",
            "aid_related               0.766890\n",
            "medical_help              0.918240\n",
            "medical_products          0.947106\n",
            "search_and_rescue         0.970822\n",
            "security                  0.983149\n",
            "military                  0.970510\n",
            "child_alone               1.000000\n",
            "water                     0.950226\n",
            "food                      0.938680\n",
            "shelter                   0.930878\n",
            "clothing                  0.983929\n",
            "money                     0.979872\n",
            "missing_people            0.988454\n",
            "refugees                  0.967546\n",
            "death                     0.961304\n",
            "other_aid                 0.867686\n",
            "infrastructure_related    0.934623\n",
            "transport                 0.955375\n",
            "buildings                 0.958028\n",
            "electricity               0.977844\n",
            "tools                     0.992667\n",
            "hospitals                 0.987830\n",
            "shops                     0.995475\n",
            "aid_centers               0.988454\n",
            "other_infrastructure      0.957092\n",
            "weather_related           0.865034\n",
            "floods                    0.945701\n",
            "storm                     0.938056\n",
            "fire                      0.988922\n",
            "earthquake                0.968638\n",
            "cold                      0.980184\n",
            "other_weather             0.946013\n",
            "direct_report             0.843345\n",
            "dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Evaluating model performance further\n",
        "Report the f1 score, precision and recall for each output category of the dataset"
      ],
      "metadata": {
        "id": "wF1zxwBmUCby"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_df = pd.DataFrame(Y_pred, columns=Y_test.columns)\n",
        "\n",
        "# Printing out the classification report for every label in the dataset\n",
        "for column in Y_test.columns:\n",
        "    print(column, classification_report(Y_test[column],y_pred_df[column]))\n",
        "    "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ITCevGbLT86z",
        "outputId": "239c0494-5b00-495f-9544-1abe3ed08910"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "related               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.68      0.41      0.51      1493\n",
            "           1       0.84      0.94      0.89      4916\n",
            "\n",
            "    accuracy                           0.82      6409\n",
            "   macro avg       0.76      0.68      0.70      6409\n",
            "weighted avg       0.80      0.82      0.80      6409\n",
            "\n",
            "request               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.98      0.93      5257\n",
            "           1       0.82      0.46      0.59      1152\n",
            "\n",
            "    accuracy                           0.89      6409\n",
            "   macro avg       0.86      0.72      0.76      6409\n",
            "weighted avg       0.88      0.89      0.87      6409\n",
            "\n",
            "offer               precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00      6378\n",
            "           1       0.00      0.00      0.00        31\n",
            "\n",
            "    accuracy                           1.00      6409\n",
            "   macro avg       0.50      0.50      0.50      6409\n",
            "weighted avg       0.99      1.00      0.99      6409\n",
            "\n",
            "aid_related               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.84      0.81      3736\n",
            "           1       0.74      0.67      0.71      2673\n",
            "\n",
            "    accuracy                           0.77      6409\n",
            "   macro avg       0.76      0.75      0.76      6409\n",
            "weighted avg       0.77      0.77      0.76      6409\n",
            "\n",
            "medical_help               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      1.00      0.96      5869\n",
            "           1       0.64      0.07      0.12       540\n",
            "\n",
            "    accuracy                           0.92      6409\n",
            "   macro avg       0.78      0.53      0.54      6409\n",
            "weighted avg       0.90      0.92      0.89      6409\n",
            "\n",
            "medical_products               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      1.00      0.97      6058\n",
            "           1       0.71      0.06      0.11       351\n",
            "\n",
            "    accuracy                           0.95      6409\n",
            "   macro avg       0.83      0.53      0.54      6409\n",
            "weighted avg       0.94      0.95      0.93      6409\n",
            "\n",
            "search_and_rescue               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      1.00      0.99      6219\n",
            "           1       0.80      0.02      0.04       190\n",
            "\n",
            "    accuracy                           0.97      6409\n",
            "   macro avg       0.89      0.51      0.51      6409\n",
            "weighted avg       0.97      0.97      0.96      6409\n",
            "\n",
            "security               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      1.00      0.99      6303\n",
            "           1       0.00      0.00      0.00       106\n",
            "\n",
            "    accuracy                           0.98      6409\n",
            "   macro avg       0.49      0.50      0.50      6409\n",
            "weighted avg       0.97      0.98      0.98      6409\n",
            "\n",
            "military               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      1.00      0.99      6216\n",
            "           1       0.70      0.04      0.07       193\n",
            "\n",
            "    accuracy                           0.97      6409\n",
            "   macro avg       0.84      0.52      0.53      6409\n",
            "weighted avg       0.96      0.97      0.96      6409\n",
            "\n",
            "child_alone               precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00      6409\n",
            "\n",
            "    accuracy                           1.00      6409\n",
            "   macro avg       1.00      1.00      1.00      6409\n",
            "weighted avg       1.00      1.00      1.00      6409\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "water               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      1.00      0.97      5988\n",
            "           1       0.88      0.28      0.43       421\n",
            "\n",
            "    accuracy                           0.95      6409\n",
            "   macro avg       0.92      0.64      0.70      6409\n",
            "weighted avg       0.95      0.95      0.94      6409\n",
            "\n",
            "food               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.99      0.97      5689\n",
            "           1       0.85      0.55      0.67       720\n",
            "\n",
            "    accuracy                           0.94      6409\n",
            "   macro avg       0.90      0.77      0.82      6409\n",
            "weighted avg       0.93      0.94      0.93      6409\n",
            "\n",
            "shelter               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.93      0.99      0.96      5818\n",
            "           1       0.82      0.32      0.46       591\n",
            "\n",
            "    accuracy                           0.93      6409\n",
            "   macro avg       0.88      0.66      0.71      6409\n",
            "weighted avg       0.92      0.93      0.92      6409\n",
            "\n",
            "clothing               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      1.00      0.99      6303\n",
            "           1       0.67      0.06      0.10       106\n",
            "\n",
            "    accuracy                           0.98      6409\n",
            "   macro avg       0.83      0.53      0.55      6409\n",
            "weighted avg       0.98      0.98      0.98      6409\n",
            "\n",
            "money               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      1.00      0.99      6274\n",
            "           1       1.00      0.04      0.09       135\n",
            "\n",
            "    accuracy                           0.98      6409\n",
            "   macro avg       0.99      0.52      0.54      6409\n",
            "weighted avg       0.98      0.98      0.97      6409\n",
            "\n",
            "missing_people               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      0.99      6335\n",
            "           1       0.00      0.00      0.00        74\n",
            "\n",
            "    accuracy                           0.99      6409\n",
            "   macro avg       0.49      0.50      0.50      6409\n",
            "weighted avg       0.98      0.99      0.98      6409\n",
            "\n",
            "refugees               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      1.00      0.98      6200\n",
            "           1       0.56      0.02      0.05       209\n",
            "\n",
            "    accuracy                           0.97      6409\n",
            "   macro avg       0.76      0.51      0.51      6409\n",
            "weighted avg       0.95      0.97      0.95      6409\n",
            "\n",
            "death               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      1.00      0.98      6139\n",
            "           1       0.89      0.09      0.17       270\n",
            "\n",
            "    accuracy                           0.96      6409\n",
            "   macro avg       0.93      0.55      0.57      6409\n",
            "weighted avg       0.96      0.96      0.95      6409\n",
            "\n",
            "other_aid               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      1.00      0.93      5558\n",
            "           1       0.54      0.02      0.05       851\n",
            "\n",
            "    accuracy                           0.87      6409\n",
            "   macro avg       0.71      0.51      0.49      6409\n",
            "weighted avg       0.83      0.87      0.81      6409\n",
            "\n",
            "infrastructure_related               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.93      1.00      0.97      5991\n",
            "           1       0.33      0.00      0.00       418\n",
            "\n",
            "    accuracy                           0.93      6409\n",
            "   macro avg       0.63      0.50      0.49      6409\n",
            "weighted avg       0.90      0.93      0.90      6409\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "transport               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      1.00      0.98      6112\n",
            "           1       0.79      0.05      0.09       297\n",
            "\n",
            "    accuracy                           0.96      6409\n",
            "   macro avg       0.87      0.52      0.54      6409\n",
            "weighted avg       0.95      0.96      0.94      6409\n",
            "\n",
            "buildings               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      1.00      0.98      6106\n",
            "           1       0.95      0.12      0.21       303\n",
            "\n",
            "    accuracy                           0.96      6409\n",
            "   macro avg       0.95      0.56      0.59      6409\n",
            "weighted avg       0.96      0.96      0.94      6409\n",
            "\n",
            "electricity               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      1.00      0.99      6266\n",
            "           1       0.67      0.01      0.03       143\n",
            "\n",
            "    accuracy                           0.98      6409\n",
            "   macro avg       0.82      0.51      0.51      6409\n",
            "weighted avg       0.97      0.98      0.97      6409\n",
            "\n",
            "tools               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      1.00      6363\n",
            "           1       0.00      0.00      0.00        46\n",
            "\n",
            "    accuracy                           0.99      6409\n",
            "   macro avg       0.50      0.50      0.50      6409\n",
            "weighted avg       0.99      0.99      0.99      6409\n",
            "\n",
            "hospitals               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      0.99      6331\n",
            "           1       0.00      0.00      0.00        78\n",
            "\n",
            "    accuracy                           0.99      6409\n",
            "   macro avg       0.49      0.50      0.50      6409\n",
            "weighted avg       0.98      0.99      0.98      6409\n",
            "\n",
            "shops               precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00      6380\n",
            "           1       0.00      0.00      0.00        29\n",
            "\n",
            "    accuracy                           1.00      6409\n",
            "   macro avg       0.50      0.50      0.50      6409\n",
            "weighted avg       0.99      1.00      0.99      6409\n",
            "\n",
            "aid_centers               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      0.99      6335\n",
            "           1       0.00      0.00      0.00        74\n",
            "\n",
            "    accuracy                           0.99      6409\n",
            "   macro avg       0.49      0.50      0.50      6409\n",
            "weighted avg       0.98      0.99      0.98      6409\n",
            "\n",
            "other_infrastructure               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      1.00      0.98      6136\n",
            "           1       0.00      0.00      0.00       273\n",
            "\n",
            "    accuracy                           0.96      6409\n",
            "   macro avg       0.48      0.50      0.49      6409\n",
            "weighted avg       0.92      0.96      0.94      6409\n",
            "\n",
            "weather_related               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.96      0.91      4621\n",
            "           1       0.85      0.63      0.72      1788\n",
            "\n",
            "    accuracy                           0.87      6409\n",
            "   macro avg       0.86      0.79      0.82      6409\n",
            "weighted avg       0.86      0.87      0.86      6409\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "floods               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      1.00      0.97      5878\n",
            "           1       0.90      0.39      0.54       531\n",
            "\n",
            "    accuracy                           0.95      6409\n",
            "   macro avg       0.92      0.69      0.76      6409\n",
            "weighted avg       0.94      0.95      0.94      6409\n",
            "\n",
            "storm               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.99      0.97      5818\n",
            "           1       0.79      0.45      0.57       591\n",
            "\n",
            "    accuracy                           0.94      6409\n",
            "   macro avg       0.87      0.72      0.77      6409\n",
            "weighted avg       0.93      0.94      0.93      6409\n",
            "\n",
            "fire               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      0.99      6338\n",
            "           1       0.00      0.00      0.00        71\n",
            "\n",
            "    accuracy                           0.99      6409\n",
            "   macro avg       0.49      0.50      0.50      6409\n",
            "weighted avg       0.98      0.99      0.98      6409\n",
            "\n",
            "earthquake               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.99      0.98      5815\n",
            "           1       0.89      0.75      0.82       594\n",
            "\n",
            "    accuracy                           0.97      6409\n",
            "   macro avg       0.93      0.87      0.90      6409\n",
            "weighted avg       0.97      0.97      0.97      6409\n",
            "\n",
            "cold               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      1.00      0.99      6278\n",
            "           1       0.70      0.05      0.10       131\n",
            "\n",
            "    accuracy                           0.98      6409\n",
            "   macro avg       0.84      0.53      0.54      6409\n",
            "weighted avg       0.97      0.98      0.97      6409\n",
            "\n",
            "other_weather               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      1.00      0.97      6056\n",
            "           1       0.77      0.03      0.05       353\n",
            "\n",
            "    accuracy                           0.95      6409\n",
            "   macro avg       0.86      0.51      0.51      6409\n",
            "weighted avg       0.94      0.95      0.92      6409\n",
            "\n",
            "direct_report               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.98      0.91      5104\n",
            "           1       0.79      0.31      0.45      1305\n",
            "\n",
            "    accuracy                           0.84      6409\n",
            "   macro avg       0.82      0.65      0.68      6409\n",
            "weighted avg       0.84      0.84      0.82      6409\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Improve the model by conducting hyper parameter tuning\n",
        "Use grid search to find better parameters for the RandomForestClassifier"
      ],
      "metadata": {
        "id": "y6lrIr4OURrH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tuning the n_estimators of the random forest classifier\n",
        "parameters = {\n",
        "        'clf1__estimator__n_estimators': [50, 100, 200]\n",
        "    }\n",
        "\n",
        "cv = GridSearchCV(pipeline, param_grid=parameters, verbose=2)\n",
        "\n",
        "cv.fit(X_train, Y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zyVhz09wUUlZ",
        "outputId": "e66c21d1-d5d6-4616-a47b-feb5e44130bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
            "[CV] END ...................clf1__estimator__n_estimators=50; total time= 5.1min\n",
            "[CV] END ...................clf1__estimator__n_estimators=50; total time= 4.7min\n",
            "[CV] END ...................clf1__estimator__n_estimators=50; total time= 4.7min\n",
            "[CV] END ...................clf1__estimator__n_estimators=50; total time= 4.6min\n",
            "[CV] END ...................clf1__estimator__n_estimators=50; total time= 4.7min\n",
            "[CV] END ..................clf1__estimator__n_estimators=100; total time= 8.2min\n",
            "[CV] END ..................clf1__estimator__n_estimators=100; total time= 8.2min\n",
            "[CV] END ..................clf1__estimator__n_estimators=100; total time= 8.0min\n",
            "[CV] END ..................clf1__estimator__n_estimators=100; total time= 8.0min\n",
            "[CV] END ..................clf1__estimator__n_estimators=100; total time= 8.0min\n",
            "[CV] END ..................clf1__estimator__n_estimators=200; total time=14.6min\n",
            "[CV] END ..................clf1__estimator__n_estimators=200; total time=14.7min\n",
            "[CV] END ..................clf1__estimator__n_estimators=200; total time=14.5min\n",
            "[CV] END ..................clf1__estimator__n_estimators=200; total time=15.5min\n",
            "[CV] END ..................clf1__estimator__n_estimators=200; total time=15.8min\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(estimator=Pipeline(steps=[('vect1',\n",
              "                                        CountVectorizer(tokenizer=<function tokenize at 0x7f29739b58c0>)),\n",
              "                                       ('tfidf1', TfidfTransformer()),\n",
              "                                       ('clf1',\n",
              "                                        MultiOutputClassifier(estimator=RandomForestClassifier()))]),\n",
              "             param_grid={'clf1__estimator__n_estimators': [50, 100, 200]},\n",
              "             verbose=2)"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Evaluate the accuracy of the tuned model\n",
        "Check the improvement in model accuracy after tuning the n_estimators"
      ],
      "metadata": {
        "id": "GCKHChxLUYBw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict on test data\n",
        "Y_pred = cv.predict(X_test)\n",
        "\n",
        "labels = np.unique(Y_pred)\n",
        "accuracy = (Y_pred == Y_test).mean()\n",
        "\n",
        "print(\"Labels:\", labels)\n",
        "print(\"Accuracy:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g80jVPetUZXf",
        "outputId": "9f61c68e-fbeb-47c4-b5b8-9e14fbc4b710"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Labels: [0 1]\n",
            "Accuracy: related                   0.818068\n",
            "request                   0.883913\n",
            "offer                     0.995163\n",
            "aid_related               0.775160\n",
            "medical_help              0.916836\n",
            "medical_products          0.947574\n",
            "search_and_rescue         0.970822\n",
            "security                  0.983305\n",
            "military                  0.970354\n",
            "child_alone               1.000000\n",
            "water                     0.953659\n",
            "food                      0.940084\n",
            "shelter                   0.931971\n",
            "clothing                  0.985177\n",
            "money                     0.979560\n",
            "missing_people            0.988454\n",
            "refugees                  0.967546\n",
            "death                     0.961929\n",
            "other_aid                 0.867218\n",
            "infrastructure_related    0.934467\n",
            "transport                 0.955531\n",
            "buildings                 0.957248\n",
            "electricity               0.978000\n",
            "tools                     0.992667\n",
            "hospitals                 0.987830\n",
            "shops                     0.995475\n",
            "aid_centers               0.988454\n",
            "other_infrastructure      0.957248\n",
            "weather_related           0.868310\n",
            "floods                    0.947106\n",
            "storm                     0.936496\n",
            "fire                      0.989078\n",
            "earthquake                0.969730\n",
            "cold                      0.979872\n",
            "other_weather             0.945545\n",
            "direct_report             0.846154\n",
            "dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Testing other algorithms and how well they perform against the RandomClassifier\n"
      ],
      "metadata": {
        "id": "BY5xk4ORUbM9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Changing the RandomClassifier to an AdaBoostClassifier\n",
        "ada_pipeline = Pipeline([\n",
        "    ('vect1', CountVectorizer(tokenizer=tokenize)),\n",
        "    ('tfidf1', TfidfTransformer()),\n",
        "    ('clf1', MultiOutputClassifier(AdaBoostClassifier()))\n",
        "])\n",
        "\n",
        "ada_parameters = {\n",
        "        'vect1__ngram_range': ((1, 1), (1, 2)), # Tune the count vectorizer for a unigram or bigram model\n",
        "        'clf1__estimator__learning_rate': [0.001, 0.0001],\n",
        "        'clf1__estimator__n_estimators': [50, 100, 150]\n",
        "    }\n",
        "\n",
        "ada_cv = GridSearchCV(ada_pipeline, param_grid=ada_parameters, verbose=2)\n",
        "ada_cv.fit(X_train, Y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y3ZHlC2Tv-xZ",
        "outputId": "7816121f-a63a-4519-dc5a-892a766ddbb7"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
            "[CV] END clf1__estimator__learning_rate=0.001, clf1__estimator__n_estimators=50, vect1__ngram_range=(1, 1); total time=  53.7s\n",
            "[CV] END clf1__estimator__learning_rate=0.001, clf1__estimator__n_estimators=50, vect1__ngram_range=(1, 1); total time=  44.2s\n",
            "[CV] END clf1__estimator__learning_rate=0.001, clf1__estimator__n_estimators=50, vect1__ngram_range=(1, 1); total time=  43.0s\n",
            "[CV] END clf1__estimator__learning_rate=0.001, clf1__estimator__n_estimators=50, vect1__ngram_range=(1, 1); total time=  42.0s\n",
            "[CV] END clf1__estimator__learning_rate=0.001, clf1__estimator__n_estimators=50, vect1__ngram_range=(1, 1); total time=  42.0s\n",
            "[CV] END clf1__estimator__learning_rate=0.001, clf1__estimator__n_estimators=50, vect1__ngram_range=(1, 2); total time= 1.0min\n",
            "[CV] END clf1__estimator__learning_rate=0.001, clf1__estimator__n_estimators=50, vect1__ngram_range=(1, 2); total time= 1.1min\n",
            "[CV] END clf1__estimator__learning_rate=0.001, clf1__estimator__n_estimators=50, vect1__ngram_range=(1, 2); total time= 1.1min\n",
            "[CV] END clf1__estimator__learning_rate=0.001, clf1__estimator__n_estimators=50, vect1__ngram_range=(1, 2); total time= 1.1min\n",
            "[CV] END clf1__estimator__learning_rate=0.001, clf1__estimator__n_estimators=50, vect1__ngram_range=(1, 2); total time= 1.0min\n",
            "[CV] END clf1__estimator__learning_rate=0.001, clf1__estimator__n_estimators=100, vect1__ngram_range=(1, 1); total time= 1.0min\n",
            "[CV] END clf1__estimator__learning_rate=0.001, clf1__estimator__n_estimators=100, vect1__ngram_range=(1, 1); total time= 1.1min\n",
            "[CV] END clf1__estimator__learning_rate=0.001, clf1__estimator__n_estimators=100, vect1__ngram_range=(1, 1); total time= 1.0min\n",
            "[CV] END clf1__estimator__learning_rate=0.001, clf1__estimator__n_estimators=100, vect1__ngram_range=(1, 1); total time= 1.0min\n",
            "[CV] END clf1__estimator__learning_rate=0.001, clf1__estimator__n_estimators=100, vect1__ngram_range=(1, 1); total time= 1.0min\n",
            "[CV] END clf1__estimator__learning_rate=0.001, clf1__estimator__n_estimators=100, vect1__ngram_range=(1, 2); total time= 1.7min\n",
            "[CV] END clf1__estimator__learning_rate=0.001, clf1__estimator__n_estimators=100, vect1__ngram_range=(1, 2); total time= 1.7min\n",
            "[CV] END clf1__estimator__learning_rate=0.001, clf1__estimator__n_estimators=100, vect1__ngram_range=(1, 2); total time= 1.7min\n",
            "[CV] END clf1__estimator__learning_rate=0.001, clf1__estimator__n_estimators=100, vect1__ngram_range=(1, 2); total time= 1.7min\n",
            "[CV] END clf1__estimator__learning_rate=0.001, clf1__estimator__n_estimators=100, vect1__ngram_range=(1, 2); total time= 1.7min\n",
            "[CV] END clf1__estimator__learning_rate=0.001, clf1__estimator__n_estimators=150, vect1__ngram_range=(1, 1); total time= 1.4min\n",
            "[CV] END clf1__estimator__learning_rate=0.001, clf1__estimator__n_estimators=150, vect1__ngram_range=(1, 1); total time= 1.4min\n",
            "[CV] END clf1__estimator__learning_rate=0.001, clf1__estimator__n_estimators=150, vect1__ngram_range=(1, 1); total time= 1.4min\n",
            "[CV] END clf1__estimator__learning_rate=0.001, clf1__estimator__n_estimators=150, vect1__ngram_range=(1, 1); total time= 1.4min\n",
            "[CV] END clf1__estimator__learning_rate=0.001, clf1__estimator__n_estimators=150, vect1__ngram_range=(1, 1); total time= 1.4min\n",
            "[CV] END clf1__estimator__learning_rate=0.001, clf1__estimator__n_estimators=150, vect1__ngram_range=(1, 2); total time= 2.4min\n",
            "[CV] END clf1__estimator__learning_rate=0.001, clf1__estimator__n_estimators=150, vect1__ngram_range=(1, 2); total time= 2.4min\n",
            "[CV] END clf1__estimator__learning_rate=0.001, clf1__estimator__n_estimators=150, vect1__ngram_range=(1, 2); total time= 2.4min\n",
            "[CV] END clf1__estimator__learning_rate=0.001, clf1__estimator__n_estimators=150, vect1__ngram_range=(1, 2); total time= 2.4min\n",
            "[CV] END clf1__estimator__learning_rate=0.001, clf1__estimator__n_estimators=150, vect1__ngram_range=(1, 2); total time= 2.4min\n",
            "[CV] END clf1__estimator__learning_rate=0.0001, clf1__estimator__n_estimators=50, vect1__ngram_range=(1, 1); total time=  41.8s\n",
            "[CV] END clf1__estimator__learning_rate=0.0001, clf1__estimator__n_estimators=50, vect1__ngram_range=(1, 1); total time=  41.8s\n",
            "[CV] END clf1__estimator__learning_rate=0.0001, clf1__estimator__n_estimators=50, vect1__ngram_range=(1, 1); total time=  41.7s\n",
            "[CV] END clf1__estimator__learning_rate=0.0001, clf1__estimator__n_estimators=50, vect1__ngram_range=(1, 1); total time=  41.6s\n",
            "[CV] END clf1__estimator__learning_rate=0.0001, clf1__estimator__n_estimators=50, vect1__ngram_range=(1, 1); total time=  41.7s\n",
            "[CV] END clf1__estimator__learning_rate=0.0001, clf1__estimator__n_estimators=50, vect1__ngram_range=(1, 2); total time= 1.0min\n",
            "[CV] END clf1__estimator__learning_rate=0.0001, clf1__estimator__n_estimators=50, vect1__ngram_range=(1, 2); total time= 1.0min\n",
            "[CV] END clf1__estimator__learning_rate=0.0001, clf1__estimator__n_estimators=50, vect1__ngram_range=(1, 2); total time= 1.0min\n",
            "[CV] END clf1__estimator__learning_rate=0.0001, clf1__estimator__n_estimators=50, vect1__ngram_range=(1, 2); total time= 1.0min\n",
            "[CV] END clf1__estimator__learning_rate=0.0001, clf1__estimator__n_estimators=50, vect1__ngram_range=(1, 2); total time= 1.0min\n",
            "[CV] END clf1__estimator__learning_rate=0.0001, clf1__estimator__n_estimators=100, vect1__ngram_range=(1, 1); total time= 1.0min\n",
            "[CV] END clf1__estimator__learning_rate=0.0001, clf1__estimator__n_estimators=100, vect1__ngram_range=(1, 1); total time= 1.0min\n",
            "[CV] END clf1__estimator__learning_rate=0.0001, clf1__estimator__n_estimators=100, vect1__ngram_range=(1, 1); total time= 1.0min\n",
            "[CV] END clf1__estimator__learning_rate=0.0001, clf1__estimator__n_estimators=100, vect1__ngram_range=(1, 1); total time= 1.1min\n",
            "[CV] END clf1__estimator__learning_rate=0.0001, clf1__estimator__n_estimators=100, vect1__ngram_range=(1, 1); total time= 1.1min\n",
            "[CV] END clf1__estimator__learning_rate=0.0001, clf1__estimator__n_estimators=100, vect1__ngram_range=(1, 2); total time= 1.7min\n",
            "[CV] END clf1__estimator__learning_rate=0.0001, clf1__estimator__n_estimators=100, vect1__ngram_range=(1, 2); total time= 1.7min\n",
            "[CV] END clf1__estimator__learning_rate=0.0001, clf1__estimator__n_estimators=100, vect1__ngram_range=(1, 2); total time= 1.7min\n",
            "[CV] END clf1__estimator__learning_rate=0.0001, clf1__estimator__n_estimators=100, vect1__ngram_range=(1, 2); total time= 1.7min\n",
            "[CV] END clf1__estimator__learning_rate=0.0001, clf1__estimator__n_estimators=100, vect1__ngram_range=(1, 2); total time= 1.7min\n",
            "[CV] END clf1__estimator__learning_rate=0.0001, clf1__estimator__n_estimators=150, vect1__ngram_range=(1, 1); total time= 1.5min\n",
            "[CV] END clf1__estimator__learning_rate=0.0001, clf1__estimator__n_estimators=150, vect1__ngram_range=(1, 1); total time= 1.5min\n",
            "[CV] END clf1__estimator__learning_rate=0.0001, clf1__estimator__n_estimators=150, vect1__ngram_range=(1, 1); total time= 1.5min\n",
            "[CV] END clf1__estimator__learning_rate=0.0001, clf1__estimator__n_estimators=150, vect1__ngram_range=(1, 1); total time= 1.5min\n",
            "[CV] END clf1__estimator__learning_rate=0.0001, clf1__estimator__n_estimators=150, vect1__ngram_range=(1, 1); total time= 1.5min\n",
            "[CV] END clf1__estimator__learning_rate=0.0001, clf1__estimator__n_estimators=150, vect1__ngram_range=(1, 2); total time= 2.5min\n",
            "[CV] END clf1__estimator__learning_rate=0.0001, clf1__estimator__n_estimators=150, vect1__ngram_range=(1, 2); total time= 2.5min\n",
            "[CV] END clf1__estimator__learning_rate=0.0001, clf1__estimator__n_estimators=150, vect1__ngram_range=(1, 2); total time= 2.5min\n",
            "[CV] END clf1__estimator__learning_rate=0.0001, clf1__estimator__n_estimators=150, vect1__ngram_range=(1, 2); total time= 2.5min\n",
            "[CV] END clf1__estimator__learning_rate=0.0001, clf1__estimator__n_estimators=150, vect1__ngram_range=(1, 2); total time= 2.5min\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(estimator=Pipeline(steps=[('vect1',\n",
              "                                        CountVectorizer(tokenizer=<function tokenize at 0x7fd20f0df440>)),\n",
              "                                       ('tfidf1', TfidfTransformer()),\n",
              "                                       ('clf1',\n",
              "                                        MultiOutputClassifier(estimator=AdaBoostClassifier()))]),\n",
              "             param_grid={'clf1__estimator__learning_rate': [0.001, 0.0001],\n",
              "                         'clf1__estimator__n_estimators': [50, 100, 150],\n",
              "                         'vect1__ngram_range': ((1, 1), (1, 2))},\n",
              "             verbose=2)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict on test data\n",
        "ada_Y_pred = ada_cv.predict(X_test)\n",
        "\n",
        "ada_labels = np.unique(ada_Y_pred)\n",
        "ada_accuracy = (ada_Y_pred == Y_test).mean()\n",
        "\n",
        "print(\"Labels:\", ada_labels)\n",
        "print(\"Accuracy:\", ada_accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bdbuwt5K0yUP",
        "outputId": "7e6b09b8-0d2a-4b4c-b547-a9ff0538b1a4"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Labels: [0 1]\n",
            "Accuracy: related                   0.661684\n",
            "request                   0.721873\n",
            "offer                     0.998820\n",
            "aid_related               0.717152\n",
            "medical_help              0.950039\n",
            "medical_products          0.969709\n",
            "search_and_rescue         0.977970\n",
            "security                  0.990559\n",
            "military                  0.995279\n",
            "water                     0.949253\n",
            "food                      0.906766\n",
            "shelter                   0.918568\n",
            "clothing                  0.993312\n",
            "money                     0.990559\n",
            "missing_people            0.992132\n",
            "refugees                  0.982691\n",
            "death                     0.974823\n",
            "other_aid                 0.864280\n",
            "infrastructure_related    0.972069\n",
            "transport                 0.979544\n",
            "buildings                 0.965775\n",
            "electricity               0.994099\n",
            "tools                     0.996066\n",
            "hospitals                 0.996066\n",
            "shops                     0.996853\n",
            "aid_centers               0.994493\n",
            "other_infrastructure      0.984658\n",
            "weather_related           0.887490\n",
            "floods                    0.977577\n",
            "storm                     0.980724\n",
            "fire                      0.995279\n",
            "earthquake                0.951220\n",
            "cold                      0.992526\n",
            "other_weather             0.979937\n",
            "direct_report             0.691188\n",
            "dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. Trying a stochastic gradient descent classifier and comparing it to other models\n",
        "\n"
      ],
      "metadata": {
        "id": "Ok92qiolP22h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Change the last step of the pipline to utilize an SGDClassifier for the MultiOutputClassifier\n",
        "sgd_pipeline = Pipeline([\n",
        "    ('vect1', CountVectorizer(tokenizer=tokenize)),\n",
        "    ('tfidf1', TfidfTransformer()),\n",
        "    ('clf1', MultiOutputClassifier(SGDClassifier()))\n",
        "])\n",
        "\n",
        "sgd_parameters = {\n",
        "        'vect1__ngram_range': ((1, 1), (1, 2)), # Tune the count vectorizer for a unigram or bigram model\n",
        "        'clf1__estimator__loss':['hinge', 'log', 'modified_huber'], # Evaluate the different algrithms governing the loss function of the SGDClassifier\n",
        "        'clf1__estimator__alpha':[0.0001,0.001] # Evaluate the best learning rate for the algorithm\n",
        "    }\n",
        "\n",
        "sgd_cv = GridSearchCV(sgd_pipeline, param_grid=sgd_parameters, verbose=2)\n",
        "sgd_cv.fit(X_train, Y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h97J--TeUdwX",
        "outputId": "65fd465e-e8ca-4943-b823-f7e8ee79d307"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
            "[CV] END clf1__estimator__alpha=0.0001, clf1__estimator__loss=hinge, vect1__ngram_range=(1, 1); total time=  15.5s\n",
            "[CV] END clf1__estimator__alpha=0.0001, clf1__estimator__loss=hinge, vect1__ngram_range=(1, 1); total time=  15.3s\n",
            "[CV] END clf1__estimator__alpha=0.0001, clf1__estimator__loss=hinge, vect1__ngram_range=(1, 1); total time=  15.3s\n",
            "[CV] END clf1__estimator__alpha=0.0001, clf1__estimator__loss=hinge, vect1__ngram_range=(1, 1); total time=  15.1s\n",
            "[CV] END clf1__estimator__alpha=0.0001, clf1__estimator__loss=hinge, vect1__ngram_range=(1, 1); total time=  15.3s\n",
            "[CV] END clf1__estimator__alpha=0.0001, clf1__estimator__loss=hinge, vect1__ngram_range=(1, 2); total time=  15.6s\n",
            "[CV] END clf1__estimator__alpha=0.0001, clf1__estimator__loss=hinge, vect1__ngram_range=(1, 2); total time=  15.6s\n",
            "[CV] END clf1__estimator__alpha=0.0001, clf1__estimator__loss=hinge, vect1__ngram_range=(1, 2); total time=  15.5s\n",
            "[CV] END clf1__estimator__alpha=0.0001, clf1__estimator__loss=hinge, vect1__ngram_range=(1, 2); total time=  15.7s\n",
            "[CV] END clf1__estimator__alpha=0.0001, clf1__estimator__loss=hinge, vect1__ngram_range=(1, 2); total time=  15.6s\n",
            "[CV] END clf1__estimator__alpha=0.0001, clf1__estimator__loss=log, vect1__ngram_range=(1, 1); total time=  15.3s\n",
            "[CV] END clf1__estimator__alpha=0.0001, clf1__estimator__loss=log, vect1__ngram_range=(1, 1); total time=  15.3s\n",
            "[CV] END clf1__estimator__alpha=0.0001, clf1__estimator__loss=log, vect1__ngram_range=(1, 1); total time=  15.3s\n",
            "[CV] END clf1__estimator__alpha=0.0001, clf1__estimator__loss=log, vect1__ngram_range=(1, 1); total time=  15.4s\n",
            "[CV] END clf1__estimator__alpha=0.0001, clf1__estimator__loss=log, vect1__ngram_range=(1, 1); total time=  15.3s\n",
            "[CV] END clf1__estimator__alpha=0.0001, clf1__estimator__loss=log, vect1__ngram_range=(1, 2); total time=  15.8s\n",
            "[CV] END clf1__estimator__alpha=0.0001, clf1__estimator__loss=log, vect1__ngram_range=(1, 2); total time=  15.9s\n",
            "[CV] END clf1__estimator__alpha=0.0001, clf1__estimator__loss=log, vect1__ngram_range=(1, 2); total time=  15.7s\n",
            "[CV] END clf1__estimator__alpha=0.0001, clf1__estimator__loss=log, vect1__ngram_range=(1, 2); total time=  15.7s\n",
            "[CV] END clf1__estimator__alpha=0.0001, clf1__estimator__loss=log, vect1__ngram_range=(1, 2); total time=  15.7s\n",
            "[CV] END clf1__estimator__alpha=0.0001, clf1__estimator__loss=modified_huber, vect1__ngram_range=(1, 1); total time=  15.1s\n",
            "[CV] END clf1__estimator__alpha=0.0001, clf1__estimator__loss=modified_huber, vect1__ngram_range=(1, 1); total time=  15.2s\n",
            "[CV] END clf1__estimator__alpha=0.0001, clf1__estimator__loss=modified_huber, vect1__ngram_range=(1, 1); total time=  15.3s\n",
            "[CV] END clf1__estimator__alpha=0.0001, clf1__estimator__loss=modified_huber, vect1__ngram_range=(1, 1); total time=  15.2s\n",
            "[CV] END clf1__estimator__alpha=0.0001, clf1__estimator__loss=modified_huber, vect1__ngram_range=(1, 1); total time=  15.1s\n",
            "[CV] END clf1__estimator__alpha=0.0001, clf1__estimator__loss=modified_huber, vect1__ngram_range=(1, 2); total time=  15.4s\n",
            "[CV] END clf1__estimator__alpha=0.0001, clf1__estimator__loss=modified_huber, vect1__ngram_range=(1, 2); total time=  15.4s\n",
            "[CV] END clf1__estimator__alpha=0.0001, clf1__estimator__loss=modified_huber, vect1__ngram_range=(1, 2); total time=  15.3s\n",
            "[CV] END clf1__estimator__alpha=0.0001, clf1__estimator__loss=modified_huber, vect1__ngram_range=(1, 2); total time=  15.4s\n",
            "[CV] END clf1__estimator__alpha=0.0001, clf1__estimator__loss=modified_huber, vect1__ngram_range=(1, 2); total time=  15.5s\n",
            "[CV] END clf1__estimator__alpha=0.001, clf1__estimator__loss=hinge, vect1__ngram_range=(1, 1); total time=  15.2s\n",
            "[CV] END clf1__estimator__alpha=0.001, clf1__estimator__loss=hinge, vect1__ngram_range=(1, 1); total time=  15.3s\n",
            "[CV] END clf1__estimator__alpha=0.001, clf1__estimator__loss=hinge, vect1__ngram_range=(1, 1); total time=  15.4s\n",
            "[CV] END clf1__estimator__alpha=0.001, clf1__estimator__loss=hinge, vect1__ngram_range=(1, 1); total time=  15.0s\n",
            "[CV] END clf1__estimator__alpha=0.001, clf1__estimator__loss=hinge, vect1__ngram_range=(1, 1); total time=  15.1s\n",
            "[CV] END clf1__estimator__alpha=0.001, clf1__estimator__loss=hinge, vect1__ngram_range=(1, 2); total time=  15.6s\n",
            "[CV] END clf1__estimator__alpha=0.001, clf1__estimator__loss=hinge, vect1__ngram_range=(1, 2); total time=  15.6s\n",
            "[CV] END clf1__estimator__alpha=0.001, clf1__estimator__loss=hinge, vect1__ngram_range=(1, 2); total time=  15.3s\n",
            "[CV] END clf1__estimator__alpha=0.001, clf1__estimator__loss=hinge, vect1__ngram_range=(1, 2); total time=  15.5s\n",
            "[CV] END clf1__estimator__alpha=0.001, clf1__estimator__loss=hinge, vect1__ngram_range=(1, 2); total time=  15.6s\n",
            "[CV] END clf1__estimator__alpha=0.001, clf1__estimator__loss=log, vect1__ngram_range=(1, 1); total time=  15.4s\n",
            "[CV] END clf1__estimator__alpha=0.001, clf1__estimator__loss=log, vect1__ngram_range=(1, 1); total time=  15.4s\n",
            "[CV] END clf1__estimator__alpha=0.001, clf1__estimator__loss=log, vect1__ngram_range=(1, 1); total time=  15.5s\n",
            "[CV] END clf1__estimator__alpha=0.001, clf1__estimator__loss=log, vect1__ngram_range=(1, 1); total time=  15.3s\n",
            "[CV] END clf1__estimator__alpha=0.001, clf1__estimator__loss=log, vect1__ngram_range=(1, 1); total time=  15.4s\n",
            "[CV] END clf1__estimator__alpha=0.001, clf1__estimator__loss=log, vect1__ngram_range=(1, 2); total time=  15.7s\n",
            "[CV] END clf1__estimator__alpha=0.001, clf1__estimator__loss=log, vect1__ngram_range=(1, 2); total time=  15.7s\n",
            "[CV] END clf1__estimator__alpha=0.001, clf1__estimator__loss=log, vect1__ngram_range=(1, 2); total time=  15.9s\n",
            "[CV] END clf1__estimator__alpha=0.001, clf1__estimator__loss=log, vect1__ngram_range=(1, 2); total time=  15.6s\n",
            "[CV] END clf1__estimator__alpha=0.001, clf1__estimator__loss=log, vect1__ngram_range=(1, 2); total time=  15.7s\n",
            "[CV] END clf1__estimator__alpha=0.001, clf1__estimator__loss=modified_huber, vect1__ngram_range=(1, 1); total time=  15.3s\n",
            "[CV] END clf1__estimator__alpha=0.001, clf1__estimator__loss=modified_huber, vect1__ngram_range=(1, 1); total time=  15.1s\n",
            "[CV] END clf1__estimator__alpha=0.001, clf1__estimator__loss=modified_huber, vect1__ngram_range=(1, 1); total time=  15.0s\n",
            "[CV] END clf1__estimator__alpha=0.001, clf1__estimator__loss=modified_huber, vect1__ngram_range=(1, 1); total time=  15.2s\n",
            "[CV] END clf1__estimator__alpha=0.001, clf1__estimator__loss=modified_huber, vect1__ngram_range=(1, 1); total time=  15.2s\n",
            "[CV] END clf1__estimator__alpha=0.001, clf1__estimator__loss=modified_huber, vect1__ngram_range=(1, 2); total time=  15.6s\n",
            "[CV] END clf1__estimator__alpha=0.001, clf1__estimator__loss=modified_huber, vect1__ngram_range=(1, 2); total time=  15.5s\n",
            "[CV] END clf1__estimator__alpha=0.001, clf1__estimator__loss=modified_huber, vect1__ngram_range=(1, 2); total time=  15.4s\n",
            "[CV] END clf1__estimator__alpha=0.001, clf1__estimator__loss=modified_huber, vect1__ngram_range=(1, 2); total time=  15.4s\n",
            "[CV] END clf1__estimator__alpha=0.001, clf1__estimator__loss=modified_huber, vect1__ngram_range=(1, 2); total time=  15.5s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(estimator=Pipeline(steps=[('vect1',\n",
              "                                        CountVectorizer(tokenizer=<function tokenize at 0x7f810a6efef0>)),\n",
              "                                       ('tfidf1', TfidfTransformer()),\n",
              "                                       ('clf1',\n",
              "                                        MultiOutputClassifier(estimator=SGDClassifier()))]),\n",
              "             param_grid={'clf1__estimator__alpha': [0.0001, 0.001],\n",
              "                         'clf1__estimator__loss': ['hinge', 'log',\n",
              "                                                   'modified_huber'],\n",
              "                         'vect1__ngram_range': ((1, 1), (1, 2))},\n",
              "             verbose=2)"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict on test data\n",
        "Y_pred = sgd_cv.predict(X_test)\n",
        "\n",
        "labels = np.unique(Y_pred)\n",
        "accuracy = (Y_pred == Y_test).mean()\n",
        "\n",
        "print(\"Labels:\", labels)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "print(\"\\nBest Parameters:\", sgd_cv.best_params_)"
      ],
      "metadata": {
        "id": "lNxX38_V0Fhn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "336fd8da-f53f-4696-958f-2152c62ffb1e"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Labels: [0 1 2]\n",
            "Accuracy: related                   0.786389\n",
            "request                   0.823761\n",
            "offer                     1.000000\n",
            "aid_related               0.822581\n",
            "medical_help              0.945712\n",
            "medical_products          0.969709\n",
            "search_and_rescue         0.982297\n",
            "security                  0.991345\n",
            "military                  0.996459\n",
            "water                     0.976397\n",
            "food                      0.958301\n",
            "shelter                   0.944532\n",
            "clothing                  0.992132\n",
            "money                     0.988198\n",
            "missing_people            0.994099\n",
            "refugees                  0.988198\n",
            "death                     0.982297\n",
            "other_aid                 0.867034\n",
            "infrastructure_related    0.968922\n",
            "transport                 0.981511\n",
            "buildings                 0.968922\n",
            "electricity               0.995673\n",
            "tools                     0.996853\n",
            "hospitals                 0.995279\n",
            "shops                     0.995673\n",
            "aid_centers               0.990952\n",
            "other_infrastructure      0.983478\n",
            "weather_related           0.925649\n",
            "floods                    0.978363\n",
            "storm                     0.981904\n",
            "fire                      0.996853\n",
            "earthquake                0.974036\n",
            "cold                      0.994886\n",
            "other_weather             0.985051\n",
            "direct_report             0.812746\n",
            "dtype: float64\n",
            "\n",
            "Best Parameters: {'clf1__estimator__alpha': 0.0001, 'clf1__estimator__loss': 'hinge', 'vect1__ngram_range': (1, 1)}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. Export the tuned model as a pickle file"
      ],
      "metadata": {
        "id": "_ITrpm7TUeKL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "with open('picked_model', 'wb') as f:\n",
        "  pickle.dump(sgd_cv, f)"
      ],
      "metadata": {
        "id": "nqc5_GNHUj1z"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "mV5q5D8GdazX"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    },
    "colab": {
      "name": "Model Development.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}